#+title: Concurrency
#+weight: 14

* Concurrency
** Introduction
The objectives of this chapter is to go in depth about the different ways to do concurrency in ZIG.

We are first going to explore a few definitions and concepts that are going to be useful to understand the rest of the chapter.

Then we are going to explore the different ways we could achieve concurrency in Zig.

By the end you should be able to see the pros and cons of each solution and choose the one that fits your needs to develop your next Zig projects.

** Definitions
Before diving into the different ways to do concurrency in ZIG, let's first define some terms that are useful to understand the basics of concurrency (not related to Zig). Some terms we are going to explain here might not even be needed in the next sections, but might help you clarify your concurency concepts.

It is important to note that the borders between some definitions are really blur and it is possible that you read a slightly different definition in another source.

*** Coroutine
Courtines enable the management of concurrent tasks like callbacks do (which make them not concurrent by nature but they are a tool to achieve concurrency). Their great power lies in their ability to write concurent task like you would write sequential code. They achieve this by yielding control between them. They are used for cooperative multitasking, since the control flow is managed by themselves and not the OS. You might see them in a lot of languages like Python, Lua, Kotlin, ... with keywords like **yield**, **resume**, **suspend**, ...

Coroutines can be either stackful or stackless, we are not gonna dive deep into this concept since most of the time you are going to use stackful coroutines since they allow you to suspend from within a nested stackframe (the only strength of stackless coroutines: efficiency)

Coroutines can also be symmetric or asymmetric:

**** Symmetric coroutines
The only way to transfer the control flow is by explicitly passing control **to another coroutine**.

**** Asymmetric coroutines (called asymmetric because the control-transfer can go both ways)
- They have two control-transfer mechanisms:
1. invoking another coroutine which is going to be the subcoroutine of the calling coroutine
2. suspending itself and giving control back to the caller

*** Green threads (userland threads)
Green threads, also known as userland threads are managed by a runtime or VM (userspace either way) instead of the OS scheduler (kernel space) that manages standard kernel threads. 

They are lightweight compared to OS threads as they have a lower overhead (since it is managed in userspace instead of kernel). 
Even though they are not real OS threads, there are still OS threads that manage them under the hood. Paritculary useful for short-lived tasks. Green threads are most likely to be cooperative and yield control between them, even though it could be possible that they are managed by a runtime that has its own scheduler and can preempt them.

Green threads might be better for multiple short lived tasks (eg: web server) because of their low overhead when context switching and memory usage.

Be careful when using green threads not to use blocking functions, because blocking 1 green thread might mean blocking all the green threads because they most likely all run on the same kernel thread.

*** Preemptive multitasking
In preemptive multitasking it is the underlying architecture (not us, but the OS or the runtime for exemple) that is in charge of choosing which threads to execute and when. This implies that our threads can be stopped (preempted) at any time, even if it is in the middle of a task. 

This method gives the advantage of not having to worry about a thread being starved, since the underlying architecture is going to make sure that everyone gets enough CPU time. As we can see in the 2 graphs for preemptive and cooperative multitasking, the preemptive multitasking might context switch our tasks too much, which can lead to a lot of overhead.

There is an interesting thing that is happening in this graph, at the end we see that task 1 is only doing a very small job, which cost a context switch for almost no job, but the scheduler does not know that a task remains only a small job until done and can context switch it.
#+CAPTION: Preemptive multitasking
#+NAME:   fig:SED-HR4049
[[./images/premp.svg]]

*** Cooperative multitasking
Contrary to preemptive multitasking, it is the progammer job to choose which and when the differents threads are executed. Threads are going to run until they are explicitly yielding control back. 

This method gives the advantage to have the progammer to have a fine grained control over his ressources, but also implies that the programmer has to think about not starving threads.

In cooperative multitasking, we can yield the control back whenever we want, which leads to much less overhead compared to preemptive multitasking.
#+CAPTION: Cooperative multitasking
#+NAME:   fig:SED-HR4049
[[./images/coop.svg]]

*** Kernel threads
Multithreading, it is the most basic and history way to do concurrency, it works by running the work on multiple threads that are going to be exectued in parallel (if the CPU can), each thread runs independently of the others. Unlike asynchronous event-driven programming, threads typically block until their assigned task completes.

Threads are managed by the OS scheduler which is going to decide when to execute which thread.

Parallelism becomes achievable through multithreading (even though its not 100% guaranteed). Threads also offer robust isolation, with each thread possessing its own execution context, stack, and local variables, ensuring task independence and preventing interference.

However, scalability can become a concern when managing numerous threads. The overhead of resource allocation by the operating system kernel for each thread may lead to scalability issues, particularly in high-demand environments. To mitigate this, thread pools are often employed to minimize the overhead of thread creation and destruction, thus optimizing performance and resource utilization.

To avoid this overhead, thread pools are often used, which manage a set of threads that can be reused for multiple tasks. This approach reduces the overhead of creating and destroying threads for each task, making it more efficient and scalable.

*** Event-driven programming
Event-driven programming, is basically an event loop that listen for "events". This architecture. Under the hood this works by having an event loop that is going to poll for events and check regulary if an event has been emitted. Those events can be for exemple interupts or signals.

*** Asynchronous programming (non-blocking IO)
Asynchronous IO can be achieved by opening non-blocking sockest and the by using one of those 2 methods:
- polling systems (epoll, kqueue, ...) that are going to poll frequently to see if a non-blocking call got its response back. Polling systems are better if there are a lot of IO operations, but less effective when less because they are going to poll for nothing most of the time.
- events (interupts, signals, ...) that are going to signal the caller that the response is is back and ready. Event-driven programming is less performant when the workload is high because interrupts have a big overhead. 
When in this mode the execution flow of the program is unkown because we don't know when a non-blocking function might be ready for use and therefore take back the control flow of the application.

This method is useful if there a lot of IO operations, so that we can start processing other things while waiting for this IO operation.

You might think that threads can do that aswell and spawn a thread each time there is a blocking call, the thread is going to be put in non-ready mode until the blocking call is done and then re-ready, the thread wakes up and yield the result for exemple. It is true threads can handle the job aswell, but the overhead of creating and managin a thread is much higher than the overhead of creating a non-blocking call. So when you have high workload, we generally prefer non-blockion IO calls.

A popular library that is used for asynchronous programming is libuv, the giant behind nodejs.

Under the hood libuv is basically a single threaded [[https://docs.libuv.org/en/v1.x/design.html#the-i-o-loop][event-loop]] which is going to perform all IOs on non-blocking sockets that are polled by pollers like epoll, kqueue, ...

** Zig solutions
There are multiple ways you currently can do concurent code in ZIG, we are going to explore a few here:

*** OS threads (std)
**** Basics
Spawning OS thread in Zig is quite simple, since it is built-in in the standard library. Here is an example of how to spawn 2 threads that are going to print numbers from 0 to x in parallel:
#+begin_src zig :imports '(std) :main 'no :testsuite 'no
  pub fn main() !void {
      //std.debug.print("Total CPU cores = {!}\n", .{std.Thread.getCpuCount()});
      const thread1 = try std.Thread.spawn(.{}, goTo, .{ 1, 5 });
      const thread2 = try std.Thread.spawn(.{}, goTo, .{ 2, 3 });
  
      thread1.join();
      thread2.join();
  }
  fn goTo(thread_id: u8, max: usize) void {
      var i: u32 = 0;
      while (i <= max) {
          std.debug.print("{} = {}\n", .{ thread_id, i });
          i += 1;
      }
  }
#+end_src
Note that the std.Thread also offer few other useful functions like `std.Thread.getCpuCount()` to get the number of CPU cores available on the machine.
#+begin_src zig :imports '(std) :main 'yes :testsuite 'no
std.debug.print("Total CPU cores = {!}\n", .{std.Thread.getCpuCount()});
#+end_src

**** Thread pool
You could also use a thread pool in order to have a few threads to multiple jobs and not 1 thread = 1 job
#+begin_src zig :imports '(std) :main 'yes :testsuite 'no
  pub fn main() !void {
      var gpa = std.heap.GeneralPurposeAllocator(.{}){};
      defer _ = gpa.deinit();
      const allocator = gpa.allocator();
  
      var pool: std.Thread.Pool = undefined;
      try pool.init(.{ .allocator = allocator, .n_jobs = 2 }); // if you dont set n_jobs it is simply going to use the total number of cores in your system, but alloactor is obligatory.
      defer pool.deinit();
  
      for (0..8) |i| {
          try pool.spawn(goTo, .{ @as(u8, @intCast(i)), 3 });
      }
  }
  
  fn goTo(thread_id: u8, max: usize) void {
      var i: u32 = 0;
      while (i <= max) {
          std.debug.print("{} = {}\n", .{ thread_id, i });
          i += 1;
      }
  }
#+end_src

**** Implementation in the std
Under the hood the threads are either pthread ([[https://ziglang.org/documentation/master/std/#std.Thread.use_pthreads][if we are under linux AND linking libc]]) or it is simpy going to use native OS threads (syscalls) wrapped by a Zig implementation. 

The advantage of doing multi-threading in Zig is that you don't have to worry about what is the target system going to be, since **std.Thread** implementation automatically chooses the native OS threads for the system your are compiling for (except if you want to enforce the use of pthreads). 

In C if you are using Windows for exemple, since **pthreads** it is not natively supported you would have to use a third-party implementation by adding a compilation tag like so:
#+begin_src c
gcc program.c -o program -pthread
#+end_src

Or worse, you would have to use a completly different library ending up with a lot of pre-processor directives to check if you are using Windows or not which is going to lead to messy code:
#+begin_src c
  #include <stdio.h>
  
  #ifdef _WIN32
  #include <windows.h>
  #else
  #include <pthread.h>
  #endif
  
  #ifdef _WIN32
  DWORD WINAPI ThreadFunc(LPVOID lpParam) {
    printf("Thread running...\n");
    return 0;
  }
  #else
  void *ThreadFunc(void *arg) {
    printf("Thread running...\n");
    return NULL;
  }
  #endif
  
  int main() {
  #ifdef _WIN32
    HANDLE hThread;
    DWORD dwThreadId;
  
    hThread = CreateThread(NULL, 0, ThreadFunc, NULL, 0, &dwThreadId);
    if (hThread == NULL) {
      printf("Failed to create thread.\n");
      return 1;
    }
  
    // Wait for the thread to finish
    WaitForSingleObject(hThread, INFINITE);
  
    // Close the thread handle
    CloseHandle(hThread);
  #else
    pthread_t thread;
    int rc;
  
    rc = pthread_create(&thread, NULL, ThreadFunc, NULL);
    if (rc) {
      printf("Failed to create thread. Return code: %d\n", rc);
      return 1;
    }
  
    // Wait for the thread to finish
    pthread_join(thread, NULL);
  #endif
  
    printf("Everything is done.\n");
  
    return 0;
  }
#+end_src

Or you could write your own wrapper kind of like the way Zig does (this is not going to work on Windows, but you get the idea):
#+begin_src c
  #include <stdio.h>
  
  #ifdef _WIN32
  #include <windows.h>
  #else
  #include <pthread.h>
  #endif
  
  int myCreate(unsigned long *thread, void *func) {
  #ifdef _WIN32
    return hThread = CreateThread(NULL, 0, func, NULL, 0, thread);
  #else
    return pthread_create(thread, NULL, func, NULL);
  #endif
  }
  
  void myJoin(unsigned long thread) {
  #ifdef _WIN32
    return WaitForSingleObject(thread, INFINITE);
  #else
    pthread_join(thread, NULL);
  #endif
  }
  
  void *ThreadFunc(void *arg) {
    printf("Thread running...\n");
    return NULL;
  }
  
  int main() {
    pthread_t thread; // TODO I should also wrap that which is pthread specific
  
    int rc = myCreate(&thread, ThreadFunc);
    if (rc) {
      printf("Failed to create thread. Return code: %d\n", rc);
      return 1;
    }
  
    myJoin(thread);
  
    printf("Everything is done.\n");
  
    return 0;
  }
#+end_src

***** Zig pthreads vs LinuxThreadImpl vs C pthreads
When compiling on Linux, by default your threads are going to use the [[https://github.com/ziglang/zig/blob/28476a5ee94d311319941b54e9da66210690ce70/lib/std/Thread.zig#L1042][LinuxThreadImpl]]. Which under the hood simply is a wrapper around some syscalls in order to manage threads (the code does closely the same thing as the pthread code).

You might have notice that when you are linking libc, Zig is going to use pthreads instead of the LinuxThreadImpl. This is because pthreads are more performant at the moment and since you are already linking libc it is better to take advantage of that and ue pt hreads.

In order to verify that we are going to benchmarks 3 different implementations: one in Zig using LinuxThreadImpl, one in Zig using pthreads and one in C using pthreads.

The way we are going to measure which implementation is better is by comparing the time it takes to *spawn* and *destory* N threads. It is useless to do work in the threads because no matter the implementation they are going to execute in the same way. It might even be counter-productive because you are going to start comparing the code inside the threads instead of the threads themselves.

Note that it is hard to benchmark thread implementations and you can easily end up not directly benchmarking them, if you for exemple try to compare the number of context switches between 2 implementations. Context switch happen *randomly* whenever the OS scheduler wants it. So trying to analyze that might lead you into false conclusions.

#+begin_src zig :imports '(std) :main 'yes :testsuite 'no
  const std = @import("std");
  
  const NB_THREADS = 10000;
  
  pub fn main() !void {
      var threads: [NB_THREADS]std.Thread = undefined;
  
      for (0..NB_THREADS) |i| {
          threads[i] = try std.Thread.spawn(.{}, goTo, .{});
      }
  
      for (0..NB_THREADS) |i| {
          threads[i].join();
      }
  }
  
  fn goTo() void {}
#+end_src

If we run this code with hyperfine (100 runs) once while linking libc (using pthreads),once in vanilla mode (using LinuxThreadImpl) and a list time using pthreads with C, we can sometimes see that there is indeed a slight performance difference between the them:
- Zig pthreads = 274.4 ms += 4.7 ms
- LinuxThreadImpl = 276.7s ms += 33.9 ms
- C pthreads =  272.7 ms += 34.0 ms

Those test have been run multiple times on different days and the results can vary a bit, but all the implementations can beat each others from time to time, since it is heavily dependent on the OS scheduler and not themselves.

The difference is so small that even when only spawning and destroying threads we barely see it. In a real world application where this would very unlikely be the bottleneck, which thread implementation you are going to use is very likely to not change anything the way your program perform.

We can then conclude that there is an almost zero cost abstraction when using threads in Zig. Which is very good for high performances applications.

**** Thread synchronization
Threads can be synchronized with utilities that are the same as most other languages (notably C). So when jumping in the [[https://ziglang.org/documentation/master/std/#std.Thread][std doc]] you should not be suprised and understand most of the features like Mutex and Semaphore.

Here is the Zig code:
#+begin_src zig :imports '(std) :main 'yes :testsuite 'no
  const std = @import("std");
  
  var common: u64 = 0;
  var m = std.Thread.Mutex{};
  
  pub fn main() !void {
      var gpa = std.heap.GeneralPurposeAllocator(.{}){};
      defer _ = gpa.deinit();
      const allocator = gpa.allocator();
  
      var pool: std.Thread.Pool = undefined;
      try pool.init(.{ .allocator = allocator });
  
      for (0..1000) |_| {
          try pool.spawn(goTo, .{});
      }
  
      pool.deinit();
  
      std.debug.print("{d}", .{common});
  }
  
  fn goTo() void {
      m.lock();
      common += 1;
      m.unlock();
  }
#+end_src

And the equivalent C code:
#+begin_src c
  #include <pthread.h>
  #include <stdio.h>
  #include <stdlib.h>
  
  #define NB_THREADS 10000
  
  pthread_mutex_t mutex;
  unsigned long long common = 0;
  
  void* goTo(void* arg) {
      pthread_mutex_lock(&mutex);
      common += 1;
      pthread_mutex_unlock(&mutex);
      return NULL;
  }
  
  int main() {
      pthread_t threads[NB_THREADS];
      int i;
  
      if (pthread_mutex_init(&mutex, NULL) != 0) {
          printf("Mutex initialization failed\n");
          return 1;
      }
  
      for (i = 0; i < NB_THREADS; i++) {
          if (pthread_create(&threads[i], NULL, goTo, NULL) != 0) {
              printf("Thread creation failed\n");
              return 1;
          }
      }
  
      for (i = 0; i < NB_THREADS; i++) {
          pthread_join(threads[i], NULL);
      }
  
      pthread_mutex_destroy(&mutex);
  
      printf("%llu\n", common);
  
      return 0;
  }
#+end_src

**** Leaky abstraction
There are 2 things you can tweak when using *std.Thread*: the stack size and the allocator.

The allocator you pass is only going to be needed only if you use the [[https://ziglang.org/documentation/master/std/#std.Thread.WasiThreadImpl][WasiThreadImpl]] (which is the default implementation for WebAssembly).
#+begin_src zig
  fn spawn(config: std.Thread.SpawnConfig, comptime f: anytype, args: anytype) SpawnError!WasiThreadImpl {
    if (config.allocator == null) {
        @panic("an allocator is required to spawn a WASI thread");
    }
    ...
  }
#+end_src

You wont't have to free it anyway since it is only used to be copied like we can see in the source code of the std.Thread:
#+begin_src zig
  // Create a copy of the allocator so we do not free the reference to the
  // original allocator while freeing the memory.
  var allocator = self.thread.allocator;
  allocator.free(self.thread.memory);
#+end_src

However, configuring the stack size is going to be used for every implementation of the threads. This is the default stack size:
#+begin_src zig
  /// Size in bytes of the Thread's stack
  stack_size: usize = 16 * 1024 * 1024
#+end_src
So if you need to modify it in order to store more local variables, pass more arguments, ... in order to avoid a stack overflow.
Don't put that value too hight either, because you might not have enough space to create a lot of threads after that. 

If you want to fine grained your thread further (eg. thread priority) you might need to use the C pthread library, which allow for a ton of possiblites of tuning. Note that when using **std.Thread** you are going to have almost everything set to the default of your implementation. For exemple the only thing that is tuned when using the **PosixThreadImpl** is the guard size.

#+begin_src zig
  assert(c.pthread_attr_setguardsize(&attr, std.mem.page_size) == .SUCCESS);
#+end_src
Which corresponds to
#+begin_src zig
  pub const page_size = switch (builtin.cpu.arch) {
      .wasm32, .wasm64 => 64 * 1024,
      .aarch64 => switch (builtin.os.tag) {
          .macos, .ios, .watchos, .tvos, .visionos => 16 * 1024,
          else => 4 * 1024,
      },
      .sparc64 => 8 * 1024,
      else => 4 * 1024,
  }
#+end_src

**** Conclusion
Zig threads are really useful since they have a very user-friendly abstraction with not a lot of functionalites that are almost never used anyway. This abstraction is also very useful for what we saw earlier, you don't have to worry about the target system, Zig is going to choose the right implementation for you.

But this leaky abstraction comes at a cost, you can not fine-tune your threads as much as you would like to.

If you need specific thread functionalities, like the ones we talked about, you can still do that in Zig by wrapping the C pthread library for exemple or directly use the OS native threads you want.

*** async/await
This method uses suspensible stackless coroutines, this solution does not necessarly mean that you are going to have multiple threads or parallelism.

We are not going to dive deeper into this solution because it has been deprecated since 0.11 and is not coming back soon.

However it is still a good reading and way to understand concurency to read this [[https://zig.guide/async/introduction/][very good guide]] that was made for this solution.
By reading this you might notice that **async/await** might **never** come out.

Note that if this solution is to be brought again it might come with breaking changes, so the syntax might change.

You can find a [[https://github.com/ziglang/zig/issues/6025][Github discussion]] about the progress of this feature and why it is not implemented in the current version.

You can see [[https://ziglang.org/download/0.12.0/release-notes.html#AsyncAwait-Feature-Status][here]] the main reasons why this solution is not implemented yet.

**** Function coloring
TODO talk about function coloring and what makes function color diaspear in zig (and talk about their relation with green threads)

*** libxev/libuv
[[https://github.com/mitchellh/libxev][Libxev]] is a Zig library that provides an event loop for non-blocking IO. The project state is "alpha-ish" to cite them from their README. 

The mains reasons behind the creation of this library 2 years ago were: using =io_uring= under the hood, and writing an event loop like libuv but in Zig. 

The first reason is not really relevant anymore, because since then libuv has implemented =io_uring= aswell so that really is not a plus anymore. 

For the second reason while being commendable and nice to use since it is native Zig, is not really a massive argument for someone who just want to have an event loop, because libuv is already a very good library that is used by a lot of people and is very stable. The only problem of libuv is that it is written in C, so you might have to do a bit of gymnastics in order to use it.

In order to use [[https://libuv.org/][libuv]], which a C library in your project we are going to have to link the libc and add the =-uv= compilation option in the =build.zig= file like so:
#+begin_src zig
  exe.linkLibC();
  exe.linkSystemLibrary("uv");
  #+end_src
  
  After that we are going to reproduce the basic exemple in C provided by the [[https://docs.libuv.org/en/v1.x/guide/basics.html][libuv documentation]] in Zig, so here is the C code:
  #+begin_src c
  #include <stdio.h>
  #include <uv.h>
  
  int main() {
      uv_loop_t *loop = uv_default_loop();
  
      printf("Default loop.\n");
      uv_run(loop, UV_RUN_DEFAULT);
  
      uv_loop_close(loop);
      return 0;
  }
#+end_src

And the equivalent Zig code:
#+begin_src zig
  const std = @import("std");
  const c = @cImport({
      @cInclude("stdio.h");
      @cInclude("uv.h");
  });
  
  pub fn main() !void {
      const loop: [*c]c.uv_loop_t = c.uv_default_loop();
  
      std.debug.print("Default loop.\n", .{});
      _ = c.uv_run(loop, c.UV_RUN_DEFAULT);
  
      _ = c.uv_loop_close(loop);
  }
#+end_src

As we can see with those exemples, it is so easy to work with a C library, so using libxev truly is not a good solution, since we can use libuv seemlessely. Moreover, libuv is a very stable library that is used by a lot of people and is very well maintained. 

On the contrary libxev does not have frequent updates, which makes it hard to use in a constantly moving Zig environment. By the time of the writing of this, there is no 0.12.0 version of libxev available yet. The PR has been made days ago but it is still not merged. Even though 0.12.0 has been out for more than a month now.

*** zigcoro
[[https://github.com/rsepassi/zigcoro][This solution]] uses stackful asymmetric coroutines.
This library is made to provide similar functionalities to async/await "old" model, so that if/when the official async/await solution is coming back, it will be easy to switch your project from using zigcoro to the official async/await. Under the hood this library uses the [[https://github.com/mitchellh/libxev][libxev]] library that we talked about earlier in order to have an event loop.

But that is not the only features that zigcoro provide, it also provides chanels.
Chanels are notably well known in Go, they are used to communicate between coroutines, they are a way pass data between them, it therefore is a good way to synchronize them.

We are going to do the same thing in Zig in order to communicate between threads without basics thread synchronization primivites like mutexes or semaphores.

First we are going to run all of them in a single-threaded environment, so that we can familiarize with the syntax.
#+begin_src zig
  const std = @import("std");
  const libcoro = @import("libcoro");
  
  const BurgerOrder = struct {
      burger: u8,
      fries: u8,
  };
  
  pub fn main() !void {
      const allocator = std.heap.page_allocator;
      var exec = libcoro.Executor.init();
      libcoro.initEnv(.{ .stack_allocator = allocator, .executor = &exec });
  
      // Creation of a Type that represents a channel that can passe floats
      const BurgeOrderChanel = libcoro.Channel(BurgerOrder, .{});
  
      // Creation of a channel that can pass Burger Orders
      var road_between_restaurant_and_house = BurgeOrderChanel.init(null);
  
      const delivery_man = try libcoro.xasync(sender, .{ &road_between_restaurant_and_house, BurgerOrder{ .burger = 2, .fries = 3 } }, null);
      defer delivery_man.deinit();
  
      const hungry_client = try libcoro.xasync(recvr, .{&road_between_restaurant_and_house}, null);
      defer hungry_client.deinit();
  
      while (exec.tick()) {
          // While there are deliveries to do, they will be made
          // After that point, the delivery man does not take any more orders
      }
  
      libcoro.xawait(delivery_man); // Delivery man finished his job
      const order = libcoro.xawait(hungry_client); // Hungry client received his order
      std.debug.print("Burger = {} | Fries = {}", .{ order.burger, order.fries });
  }
  
  fn sender(chan: anytype, order: BurgerOrder) void {
      defer chan.close();
      chan.send(order) catch unreachable;
  }
  
  fn recvr(chan: anytype) BurgerOrder {
      return chan.recv() orelse BurgerOrder{ .burger = 0, .fries = 0 }; // The delivery might fail to arrive
  }
#+end_src

I tried to make inter-thread communication with chanels by modifying this exemple but I could not do it. I opened [[https://github.com/rsepassi/zigcoro/issues/22][an issue]] on the zigcoro repository to ask for help. I am waiting for a response before making any conclusion.

Zigcoro is only maintained by 2 people, even though they still update frequently for the new zig versions, the library has not evolved for a while and there are some PR that are just hanging there for a while. If chanels can work to make inter-thread synchronization possible, it might be a good library.

*** Using other C libraries
Obviously you can still use any C library of your choice to do you the job.

** Conclusion
Almost each of the presented solutions have their own specific applications and are not really interchangeable. The solution that is going to be the best for most of the projects is simply spawning threads, even though they have a quite big overhead, if the application doesn't have hundred of threads running at the same time, it should largely do the job.

async/await is a great feature, but since it is not supported anymore it is cleary not a viable option at the moment.

libuv might be the solution if the needed application is single-threaded and leverages non-blocking sockets, particulary useful for servers that have to handle massive loads of IO operations.

TODO for the case of zigoro it is going to depend on what the maintainer is going to say about the issues I opened.

** Sources:
- https://dl.acm.org/doi/pdf/10.1145/1462166.1462167
- https://www.lua.org/pil/9.1.html
- https://blog.orhun.dev/zig-bits-04/ (regarder regul pour voir si il m a rep)
- https://github.com/mitchellh/libxev/issues/92 (regul ...)
- ChatGPT et GH Copilot
- https://github.com/lewissbaker/cppcoro
- https://ericniebler.com/2020/11/08/structured-concurrency/
- https://en.wikipedia.org/wiki/Fiber_(computer_science)
- https://github.com/rsepassi/zigcoro
- https://github.com/mitchellh/libxev
- https://github.com/libuv/libuv
- https://github.com/libuv/libuv/blob/v1.x/docs/src/guide/basics.rst
- https://docs.libuv.org/en/v1.x/guide/basics.html (ptetre le meme que celui au dessus)
- https://github.com/dotnet/runtimelab/issues/2398
- https://kristoff.it/blog/zig-colorblind-async-await/
- https://tigerbeetle.com/blog/a-friendly-abstraction-over-iouring-and-kqueue/
- https://docs.libuv.org/en/v1.x/design.html#the-i-o-loop
- https://docs.libuv.org/en/v1.x/guide/threads.html
- https://softwareengineering.stackexchange.com/questions/254140/is-there-a-difference-between-fibers-coroutines-and-green-threads-and-if-that-i
- https://github.com/ziglang/zig/issues/6025
- https://www.reddit.com/r/Zig/comments/177e4cb/what_are_you_doing_for_async/
- https://github.com/catdevnull/awesome-zig?tab=readme-ov-file#network
- https://stackoverflow.com/questions/41891989/what-is-the-difference-between-asymmetric-and-symmetric-coroutines
- https://www.baeldung.com/java-threading-models#:~:text=The%20big%20difference%20between%20green,executing%20at%20any%20given%20time.
- https://stackoverflow.com/questions/28977302/how-do-stackless-coroutines-differ-from-stackful-coroutines
